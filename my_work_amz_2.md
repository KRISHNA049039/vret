Skip to content
Chat history




You said:
How to use cdk to deploy lamda function code?
ChatGPT said:
To deploy a Lambda function using AWS CDK (Cloud Development Kit), you define your Lambda in code (Python, TypeScript, etc.), specify the source of the function code, and then deploy it with the cdk deploy command.

‚úÖ 1. Install AWS CDK
npm install -g aws-cdk
‚úÖ 2. Create a New CDK Project
Choose your preferred language (TypeScript or Python below):

TypeScript
cdk init app --language=typescript
Python
cdk init app --language=python
source .venv/bin/activate
pip install -r requirements.txt
‚úÖ 3. Install Lambda Construct Library
TypeScript:
npm install @aws-cdk/aws-lambda
Python:
pip install aws-cdk.aws-lambda
‚úÖ 4. Create Your Lambda Code
Create a file lambda/hello.py:

def handler(event, context):
    return {
        'statusCode': 200,
        'body': 'Hello from Lambda!'
    }
‚úÖ 5. Define the Lambda in CDK Stack
TypeScript Example (in lib/my-stack.ts)
import * as cdk from 'aws-cdk-lib';
import * as lambda from 'aws-cdk-lib/aws-lambda';

export class MyStack extends cdk.Stack {
  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    new lambda.Function(this, 'MyLambda', {
      runtime: lambda.Runtime.PYTHON_3_9,
      handler: 'hello.handler',
      code: lambda.Code.fromAsset('lambda'),
    });
  }
}
Python Example (in my_stack.py)
from aws_cdk import (
    Stack,
    aws_lambda as _lambda,
)
from constructs import Construct

class MyStack(Stack):
    def __init__(self, scope: Construct, id: str, **kwargs):
        super().__init__(scope, id, **kwargs)

        _lambda.Function(
            self, 'MyLambda',
            runtime=_lambda.Runtime.PYTHON_3_9,
            handler='hello.handler',
            code=_lambda.Code.from_asset('lambda')
        )
‚úÖ 6. Deploy the Stack
cdk deploy
‚úÖ Summary
Component	Description
code.fromAsset	Points to folder containing .py, .js, etc.
handler	Format: <filename>.<function>
runtime	e.g. PYTHON_3_9, NODEJS_18_X
cdk deploy	Deploys your infrastructure to AWS
If you have a ZIP file or S3 bucket, I can also show how to deploy from that source. Let me know!

You said:
 glap_query = """SELECT ledger_name,company_code,location_code,cost_center,account_number,product_code,channel_code,project_code,journal_name,
    line_description,gl_posted_date,period_name,functional_currency_code,functional_amount,entered_currency_code,entered_amount,
    invoice_image,invoice_num,vendor_name,vendor_number,je_source,je_category from fast.vw_gl_ap_details    [CPython39-glue_local_run] app-1                  |     raise e    
[CPython39-glue_local_run] app-1                  | ['/home/glue_user/workspace/configuration/glue_entrypoints/fdatanetics_entrypoint.py', '--file_id', '123485697', '--job_name', 'vret_rvr', '--app_id', '265', '--triggered_by', 'krkottap', '--stage', 'BETA', '--run_type', 'flex_run', '--app_id', '123', '--Geo', 'US', '--flex_id', '187']
[CPython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/configuration/glue_entrypoints/fdatanetics_entrypoint.py", line 44, in <module>
[CPython39-glue_local_run] app-1                  | US
[CPython39-glue_local_run] app-1                  |     process = vret_run(args)    
[CPython39-glue_local_run] app-1                  | Read in VRET Data
[CPython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/localrun/glue_scripts/fdatanetics/vret_automation_fdnt/vret.py", line 130, in runProcess
[CPython39-glue_local_run] app-1                  |     ledger_query=ledger_query.replace('\n', '')
[CPython39-glue_local_run] app-1                  | UnboundLocalError: local variable 'ledger_query' referenced before assignment 
    WHERE upper(period_name)='"""+cur_date.strftime('%b-%y').upper()+"""' and je_category IN ('Purchase Invoices','Sales Invoices') 
    and account_number='20321' AND ledger_name='Amazon.eu' AND company_code='6L' and channel_code='"""+led2chan[ledname]+"""'"""
        
        ledger_query="""SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""

        rvr_vret_de = """SELECT * FROM fast.vret_rvr WHERE country_code IN ('DE','CZ')"""

        rvr_vret ="""SELECT * FROM fast.vret_rvr"""
    
    cm_query = cm_query.replace('\n','')
    adj_query = adj_query.replace('\n','')
    glap_query=glap_query.replace('\n','')
    ledger_query=ledger_query.replace('\n', '')
    rvr_vret_de=rvr_vret_de.replace('\n', '')
    rvr_vret=rvr_vret.replace('\n', '')
    adj_query_instance= DatabaseUtility((args['stage'].upper()),'redshift')
    adjs = adj_query_instance.execute_query('adj_query')
ChatGPT said:
The error you're encountering:

UnboundLocalError: local variable 'ledger_query' referenced before assignment
means you're trying to use ledger_query before it's defined.

üîç Root Cause:
You're using:

ledger_query = ledger_query.replace('\n', '')
before you've defined:

ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
This is not allowed in Python ‚Äî a variable must be defined before it's used.

‚úÖ Fix:
Move this line:

ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
before you call:

ledger_query = ledger_query.replace('\n', '')
‚úî Correct Order:
ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
ledger_query = ledger_query.replace('\n', '')
Do the same for all other queries like glap_query, cm_query, etc.

üí° Optional Tip:
To make it cleaner, you can write a small helper:

def clean_query(query):
    return query.replace('\n', ' ').strip()

ledger_query = clean_query("""
    SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers
""")

glap_query = clean_query(f"""
    SELECT ledger_name, company_code, location_code, cost_center, account_number, product_code,
           channel_code, project_code, journal_name, line_description, gl_posted_date, period_name,
           functional_currency_code, functional_amount, entered_currency_code, entered_amount,
           invoice_image, invoice_num, vendor_name, vendor_number, je_source, je_category
    FROM fast.vw_gl_ap_details
    WHERE upper(period_name)='{cur_date.strftime('%b-%y').upper()}'
      AND je_category IN ('Purchase Invoices','Sales Invoices')
      AND account_number='20321'
      AND ledger_name='Amazon.eu'
      AND company_code='6L'
      AND channel_code='{led2chan[ledname]}'
""")
Let me know if you'd like help modularizing or refactoring more of this Glue code!

You said:
but in my code its done
ChatGPT said:
Thanks for pointing that out. Let‚Äôs re-analyze this, since you said the assignment for ledger_query is done before the .replace('\n', '') line.

üìå You shared:
glap_query = """SELECT ledger_name, ..."""

...

ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""

...

glap_query=glap_query.replace('\n','')
ledger_query=ledger_query.replace('\n', '')
But look closely at this part:

[CPython39-glue_local_run] app-1 |   File "/home/glue_user/workspace/localrun/glue_scripts/fdatanetics/vret_automation_fdnt/vret.py", line 130, in runProcess
[CPython39-glue_local_run] app-1 |     ledger_query=ledger_query.replace('\n', '')
[CPython39-glue_local_run] app-1 | UnboundLocalError: local variable 'ledger_query' referenced before assignment
üîé Diagnosis
Even though you declared ledger_query earlier, Python is complaining because of how the scope works.

This usually happens when:

‚úÖ ledger_query is used inside a function, but:
Defined outside, or

There is another local assignment after usage, making Python think it‚Äôs local even before the assignment.

üí° What‚Äôs Likely Happening
If you're doing this inside a function like def runProcess():, and somewhere later in the function you do:

ledger_query = something
Then Python treats ledger_query as a local variable throughout the function, including before it's actually assigned.

So this line causes the issue:

ledger_query = ledger_query.replace('\n', '')
Because Python assumes ledger_query is local to the function, but it hasn‚Äôt yet been assigned.

‚úÖ Solutions
Option 1: Move the assignment to the top of the function
Make sure ledger_query = ... comes before any use of ledger_query in the same function.

Option 2: Avoid reassigning
If you're only modifying, use a new variable:

ledger_query_cleaned = ledger_query.replace('\n', '')
Option 3: Declare as global (not recommended unless intentional)
If the query is declared outside the function, and you really want to use it inside:

global ledger_query
ledger_query = ledger_query.replace('\n', '')
üîÅ Example Fix:
def runProcess():
    ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
    ledger_query = ledger_query.replace('\n', '')
If you're still unsure, feel free to paste the full function or class definition (or at least the area around runProcess) and I‚Äôll pinpoint the exact fix.

You said:
def runProcess(args):
    '''
    Runs the Vendor Funding Process
    '''
    #infile = pd.read_excel(args['Geo'])
    l2c = {'Amazon.ca':'CA','Amazon.com, Inc.':'US', 'Amazon.mx':'MX','Amazon.co.jp':'JP','Amazon.br':'BR', 'Amazon.ae':'AE','Amazon.co.uk Ltd.':'UK','Amazon.eu':'EU','Amazon.com.au':'AU','Amazon.sg':'SG','Amazon.de.eur':'DE',
           'Amazon.es.eur':'ES','Amazon.cz':'CZ','Amazon.fr.eur':'FR','Amazon.in':'IN','Amazon.it':'IT','Amazon.nl.eur':'NL','Amazon.pl':'PL','Amazon.ron':'RO','Amazon.sa':'ZA','Amazon.sk':'SK','Amazon.tr':'TR','Amazon.ksa':'SA','Amazon.eg':'EG','Amazon.eu':'SE'}
    c2l = {v:k for k,v in l2c.items()}
    led2chan = {'CA':'1030','JP':'1025','MX':'1092','ES':'1040','DE':'1015','FR':'1020','IT':'1035','AE':'1096','US':'1000','BR':'1090','UK':'0000','EU':'0000','AU':'1091','SG':'1022','CZ':'0000',
                'IN':'0000','NL':'1043','PL':'1018','RO':'0000','SK':'0000','TR':'0000','SA':'1097','EG':'1098','SE':'1034'}
    ledname=args['Geo']
    ledger = c2l[ledname]
    temp_date = datetime.datetime.now()
    cur_date = datetime.datetime(temp_date.year,temp_date.month,1)-datetime.timedelta(days=1)
    print('Read in VRET Data')
    if ledname not in ['DE','ES','IT','SE','PL','FR']:
        cm_query = """with WO as (select DISTINCT source_id_int_1 as wo_source_id_int_1, distribution_id as wo_distribution_id, source_distribution_id as wo_source_distribution_id, 
    segment4 as wo_account_number,segment1 as wo_company_code,segment2 as wo_location_code,segment3 as wo_cost_center,
    segment5 as wo_product_line,segment6 as wo_channel,segment7 as wo_project,segment8 as wo_segment8,
    je_category as wo_je_category,ledger_name as wo_ledger_name,period_name as wo_period_name,
    unrounded_accounted as wo_unrounded_accounted from fast.gl_ar_linkage where segment4 = '20321'
    and ledger_name='"""+ledger+"""' and je_category = 'Credit Memos' AND upper(period_name)='"""+cur_date.strftime('%b-%y').upper()+"""'),
    AR as (select DISTINCT party_name as ar_vendor,invoice_number as ar_invoice_num,invoice_date as ar_invoice_date,
    customer_trx_id as ar_cust_trx_id,inv_cur_amount_adjusted as ar_inv_amount_adjust,
    led_cur_amount_due_original as ar_original_led_cur_amount,led_cur_amount_due_remaining as ar_remaining_led_cur_amount,
    product_code as ar_product_line,source_distribution_id as ar_source_distribution_id,ledger_name as ar_ledger_name from ateam.hoot.confluence_ar_invoice_all_view 
    where account_number IN ('20320','13113') AND ledger_name='"""+ledger+"""'),
    BASE as (select * from WO left join AR on wo.wo_source_id_int_1 = ar.ar_cust_trx_id)
    select wo_source_id_int_1, wo_distribution_id, wo_source_distribution_id, wo_ledger_name,wo_period_name,
    wo_je_category,wo_company_code,wo_location_code,wo_cost_center,wo_account_number,wo_product_line,
    wo_channel,wo_project,wo_segment8,wo_unrounded_accounted,ar_cust_trx_id,ar_invoice_date,ar_invoice_num,
    ar_ledger_name,ar_vendor,ar_product_line,ar_original_led_cur_amount, ar_remaining_led_cur_amount from BASE"""

        adj_query="""with WO as (select DISTINCT source_id_int_1 as wo_source_id_int_1,distribution_id as wo_distribution_id,
    source_distribution_id as wo_source_distribution_id,segment4 as wo_account_number,segment1 as wo_company_code,
    segment2 as wo_location_code,segment3 as wo_cost_center,segment5 as wo_product_line,segment6 as wo_channel,
    segment7 as wo_project,segment8 as wo_segment8,je_category as wo_je_category,ledger_name as wo_ledger_name,
    period_name as wo_period_name,unrounded_accounted as wo_unrounded_accounted from fast.gl_ar_linkage where segment4 = '20321' and 
    ledger_name='"""+ledger+"""' and je_category = 'Adjustment' AND upper(period_name)='"""+cur_date.strftime('%b-%y').upper()+"""'),
    AR as (select DISTINCT customer_name as ar_vendor,invoice_number as ar_invoice_num,invoice_date as ar_invoice_date,
    adjustment_id as ar_adjustment_id, adjustment_amount as ar_adjustment_amount, product_code as ar_product_code,
    source_distribution_id as ar_source_distribution_id, ledger_name as ar_ledger_name from fdv_ext.ar_adjustment_distribution_details where account_number  IN ('20320','13113') 
    AND ledger_name='"""+ledger+"""'), BASE as (select * from WO left join AR on 
    wo.wo_source_id_int_1 = ar.ar_adjustment_id) select wo_source_id_int_1, wo_distribution_id, 
    wo_source_distribution_id,wo_ledger_name,wo_period_name, wo_je_category, wo_company_code,wo_location_code, 
    wo_cost_center, wo_account_number, wo_product_line, wo_channel, wo_project, wo_segment8, wo_unrounded_accounted,
    ar_adjustment_id, ar_invoice_date, ar_invoice_num, ar_ledger_name, ar_vendor, ar_product_code, 
    ar_adjustment_amount, ar_source_distribution_id from BASE"""

        glap_query = """SELECT ledger_name,company_code,location_code,cost_center,account_number,product_code,channel_code,project_code,journal_name,
    line_description,gl_posted_date,period_name,functional_currency_code,functional_amount,entered_currency_code,entered_amount,
    invoice_image,invoice_num,vendor_name,vendor_number,je_source,je_category from fast.vw_gl_ap_details 
    WHERE upper(period_name)='"""+cur_date.strftime('%b-%y').upper()+"""' and je_category IN ('Purchase Invoices','Sales Invoices') and account_number='20321' and ledger_name='"""+ledger+"""';"""
    else:
        cm_query = """with WO as (select DISTINCT source_id_int_1 as wo_source_id_int_1, distribution_id as wo_distribution_id, source_distribution_id as wo_source_distribution_id, 
    segment4 as wo_account_number,segment1 as wo_company_code,segment2 as wo_location_code,segment3 as wo_cost_center,
    segment5 as wo_product_line,segment6 as wo_channel,segment7 as wo_project,segment8 as wo_segment8,
    je_category as wo_je_category,ledger_name as wo_ledger_name,period_name as wo_period_name,
    unrounded_accounted as wo_unrounded_accounted from fast.gl_ar_linkage where segment4 = '20321'
    and segment1='6L' and ledger_name='Amazon.eu' and segment6='"""+led2chan[ledname]+"""' and je_category = 'Credit Memos' AND upper(period_name)='"""+cur_date.strftime('%b-%y').upper()+"""'),
    AR as (select DISTINCT party_name as ar_vendor,invoice_number as ar_invoice_num,invoice_date as ar_invoice_date,
    customer_trx_id as ar_cust_trx_id,inv_cur_amount_adjusted as ar_inv_amount_adjust,
    led_cur_amount_due_original as ar_original_led_cur_amount,led_cur_amount_due_remaining as ar_remaining_led_cur_amount,
    product_code as ar_product_line,source_distribution_id as ar_source_distribution_id,ledger_name as ar_ledger_name from ateam.hoot.confluence_ar_invoice_all_view 
    where account_number IN ('20320','13113') AND ledger_name='Amazon.eu'), BASE as (select * from WO left join AR 
    on wo.wo_source_id_int_1 = ar.ar_cust_trx_id) select wo_source_id_int_1, wo_distribution_id, wo_source_distribution_id, wo_ledger_name,wo_period_name,
    wo_je_category,wo_company_code,wo_location_code,wo_cost_center,wo_account_number,wo_product_line,
    wo_channel,wo_project,wo_segment8,wo_unrounded_accounted,ar_cust_trx_id,ar_invoice_date,ar_invoice_num,
    ar_ledger_name,ar_vendor,ar_product_line,ar_original_led_cur_amount, ar_remaining_led_cur_amount from BASE"""

        adj_query="""with WO as (select DISTINCT source_id_int_1 as wo_source_id_int_1,distribution_id as wo_distribution_id,
    source_distribution_id as wo_source_distribution_id,segment4 as wo_account_number,segment1 as wo_company_code,
    segment2 as wo_location_code,segment3 as wo_cost_center,segment5 as wo_product_line,segment6 as wo_channel,
    segment7 as wo_project,segment8 as wo_segment8,je_category as wo_je_category,ledger_name as wo_ledger_name,
    period_name as wo_period_name,unrounded_accounted as wo_unrounded_accounted from fast.gl_ar_linkage where segment4 = '20321' and 
    ledger_name='Amazon.eu' and segment1='6L' and segment6='"""+led2chan[ledname]+"""' and je_category = 'Adjustment' AND upper(period_name)='"""+cur_date.strftime('%b-%y').upper()+"""'),
    AR as (select DISTINCT customer_name as ar_vendor,invoice_number as ar_invoice_num,invoice_date as ar_invoice_date,
    adjustment_id as ar_adjustment_id, adjustment_amount as ar_adjustment_amount, product_code as ar_product_code,
    source_distribution_id as ar_source_distribution_id, ledger_name as ar_ledger_name from fdv_ext.ar_adjustment_distribution_details where account_number  IN ('20320','13113')
    and ledger_name='Amazon.eu'), BASE as (select * from WO left join AR on wo.wo_source_id_int_1 = ar.ar_adjustment_id) select wo_source_id_int_1, wo_distribution_id, 
    wo_source_distribution_id,wo_ledger_name,wo_period_name, wo_je_category, wo_company_code,wo_location_code, 
    wo_cost_center, wo_account_number, wo_product_line, wo_channel, wo_project, wo_segment8, wo_unrounded_accounted,
    ar_adjustment_id, ar_invoice_date, ar_invoice_num, ar_ledger_name, ar_vendor, ar_product_code, 
    ar_adjustment_amount, ar_source_distribution_id from BASE"""

        glap_query = """SELECT ledger_name,company_code,location_code,cost_center,account_number,product_code,channel_code,project_code,journal_name,
    line_description,gl_posted_date,period_name,functional_currency_code,functional_amount,entered_currency_code,entered_amount,
    invoice_image,invoice_num,vendor_name,vendor_number,je_source,je_category from fast.vw_gl_ap_details 
    WHERE upper(period_name)='"""+cur_date.strftime('%b-%y').upper()+"""' and je_category IN ('Purchase Invoices','Sales Invoices') 
    and account_number='20321' AND ledger_name='Amazon.eu' AND company_code='6L' and channel_code='"""+led2chan[ledname]+"""'"""
        
        ledger_query="""SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""

        rvr_vret_de = """SELECT * FROM fast.vret_rvr WHERE country_code IN ('DE','CZ')"""

        rvr_vret ="""SELECT * FROM fast.vret_rvr"""
    
    cm_query = cm_query.replace('\n','')
    adj_query = adj_query.replace('\n','')
    glap_query=glap_query.replace('\n','')
    ledger_query=ledger_query.replace('\n', '')
    rvr_vret_de=rvr_vret_de.replace('\n', '')
    rvr_vret=rvr_vret.replace('\n', '')
    adj_query_instance= DatabaseUtility((args['stage'].upper()),'redshift')
    adjs = adj_query_instance.execute_query('adj_query')

    adjs.columns = [a.decode('utf-8') if isinstance(a,bytes) else a for a in list(adjs.columns)]
    adjs=adjs[['wo_ledger_name','wo_period_name','wo_company_code','wo_location_code','wo_cost_center',
                  'wo_account_number','wo_product_line','wo_channel','wo_project','wo_unrounded_accounted','ar_vendor','ar_invoice_num','wo_je_category']]
    adjs['wo_period_name']=[a.upper() for a in list(adjs['wo_period_name'])]
    cms_instance = DatabaseUtility((args['stage'].upper()), 'redshift')
    cms = cms_instance.execute_query('cm_query')
    cms.columns = [a.decode('utf-8') if isinstance(a,bytes) else a for a in list(cms.columns)]
    cms = cms[['wo_ledger_name','wo_period_name','wo_company_code','wo_location_code','wo_cost_center',
                 'wo_account_number','wo_product_line','wo_channel','wo_project','wo_unrounded_accounted','ar_vendor','ar_invoice_num','wo_je_category']]
    cms['wo_cost_center']=cms['wo_cost_center'].map(str)
    cms['wo_period_name']=[a.upper() for a in list(cms['wo_period_name'])]
    cms = cms.append(adjs).reset_index(drop=True)
    #s3 = boto3.client('s3')
    #s3bucket = os.environ['OutputBucketName']
    #creds1 = secrets.get_secret('AlteryxRedshiftv2', 'us-west-2')
    #conn = pg8000.connect(user = creds1['Username'], password = creds1['Password'], host =creds1['Host'],  port = int(creds1['Port']), database = creds1['Database'],ssl_context=True)
    print('Read in VRET Data')
    pis_instance= DatabaseUtility((args['stage'].upper()), 'redshift')
    pis= pis_instance.execute_query('glap_query')
    pis.columns = [a.decode('utf-8') if isinstance(a,bytes) else a for a in list(pis.columns)]
    pis['period_name']=[a.upper() for a in list(pis['period_name'])]
    pis['cost_center']=pis['cost_center'].map(str)
    cd = datetime.datetime(cur_date.year,cur_date.month,1)
    s3_resource = boto3.resource('s3')
    
    led_instance=DatabaseUtility((args['stage'].upper()), 'redshift')
    leds=led_instance.execute_query('ledger_query')

    #leds = pd.read_sql('SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers',conn)
    leds.columns = [a.decode('utf-8') if isinstance(a,bytes) else a for a in list(leds.columns)]
    newleds = leds[['ledger_id','ledger_name','currency_code']]
    newleds.columns = ['LEDGER_ID','Ledger','Currency']     
    print('Ledger: ', ledger)
    templed = ledger.upper()
    templed = templed.replace(' ','_')
    if ledname not in ['US','CA','MX','BR']:
        source='com.amazon.ofa.ar.rvr-thr.reports.beta'
        bucket = s3_resource.Bucket(source)
        keys=[]
        for b in bucket.objects.all():
            keys.append(b.key) 
        print(keys)
        tempkeys = [b for b in keys if templed in b.upper()]
        #print(tempkeys)
        #rvrkeys = [t for t in tempkeys if 'rvr' in t]
        if ledname=='DE':
            dekeys = [t for t in keys if 'thr/Amazon.eu_DE' in t]
            czkeys = [t for t in keys if 'thr/Amazon.eu_CZ' in t]
            plkeys = [t for t in keys if 'thr/Amazon.eu_PL' in t]
            dekey = '' if len(dekeys)==0 else max(dekeys)
            czkey = '' if len(czkeys)==0 else max(czkeys)
            plkey = '' if len(czkeys)==0 else max(plkeys)
            thrkeys=[]
            print('DE KEY: ', dekey)
            print('CZ KEY: ', czkey)
            print('PL KEY: ', plkey)
        elif ledname in ['ES','UK','IT','DE','CZ','PL','FR']:
            thrkeys = [t for t in keys if 'thr/Amazon.eu_'+l2c[ledger] in t]
        else:
            thrkeys = [t for t in tempkeys if 'thr' in t]
        thrkey = '' if len(thrkeys)==0 else max(thrkeys)
        print(thrkey)
    print('RUNNING RVR')
    if ledname=='DE':
        rvr_vret_de_instance=DatabaseUtility((args['stage'].upper()), 'redshift')
        rvr = rvr_vret_de_instance.execute_query('rvr_vret_de')
        #rvr = pd.read_sql("SELECT * FROM fast.vret_rvr WHERE country_code IN ('DE','CZ')",conn)
    else:
        rvr_vret_instance=DatabaseUtility((args['stage'].upper()), 'redshift')
        rvr = rvr_vret_instance.execute_query('rvr_vret')
        #rvr = pd.read_sql("SELECT * FROM fast.vret_rvr",conn)
    rvr.columns = [a.decode('utf-8') if isinstance(a,bytes) else a for a in list(rvr.columns)]
    rvr.columns = [a.upper() for a in list(rvr.columns)]
    rvr.columns = [a.replace('_',' ') for a in list(rvr.columns)]
    rvr['RVR DATE']=[a[:10] for a in list(rvr['RVR DATE'])]
    rvr['RVR DATE']=[datetime.datetime.strptime(a,'%Y-%m-%d') for a in list(rvr['RVR DATE'])]
    rvr['COUNTRY CODE'] = ['UK' if a=='GB' else a for a in list(rvr['COUNTRY CODE'])]
ChatGPT said:
Thanks for sharing the full function runProcess(args). Based on your code, I now clearly see why you're still getting:

UnboundLocalError: local variable 'ledger_query' referenced before assignment
Even though you are assigning ledger_query, the error happens because of how conditional blocks work.

üî• Root Cause
The assignment:

ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
is inside this block:

else:
    ...
    ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
Which means: if the condition does not go into that else block, ledger_query is never assigned at all.

Then later, this line is unconditional:

ledger_query = ledger_query.replace('\n', '')
If the else block didn't execute, you're trying to .replace() on a variable that was never defined.

‚úÖ Solution
To fix this:

Option 1: Move ledger_query definition outside the if-else
If it's static and the same for all paths:

ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
ledger_query = ledger_query.replace('\n', '')
Option 2: Declare it in both if and else blocks
Right now you only define it in the else. You can add the same definition inside the if block too.

if ledname not in ['DE','ES','IT','SE','PL','FR']:
    ...
    ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
else:
    ...
    ledger_query = """SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers"""
Option 3: Initialize with empty default (if optional)
ledger_query = ""
if ledname not in [...]:
    ...
elif ...
    ledger_query = ...
ledger_query = ledger_query.replace('\n', '')  # safe now
‚úÖ Final Tip: Safer Query Rewriting
Instead of rewriting with .replace('\n', '') everywhere, consider a helper function:

def clean_sql(query):
    return ' '.join(query.strip().splitlines())

ledger_query = clean_sql("""
    SELECT * FROM frisco.ateam_fixed_assets_fdatanetics_gl_ledgers
""")
This avoids extra space, preserves safety, and looks cleaner.

Let me know if you want help modularizing your SQL construction, or loading them from files dynamically.

You said:
Python39-glue_local_run] app-1                  | US
[CPython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/configuration/glue_entrypoints/fdatanetics_entrypoint.py", line 44, in <module>
[CPython39-glue_local_run] app-1                  | Read in VRET Data
[CPython39-glue_local_run] app-1                  |     process = vret_run(args)    
[CPython39-glue_local_run] app-1                  | <glue_scripts.automation_libraries.redshift.DatabaseUtility object at 0x7effa01fc0d0>
[CPython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/localrun/glue_scripts/fdatanetics/vret_automation_fdnt/vret.py", line 143, in runProcess
[CPython39-glue_local_run] app-1                  |     adjs = adj_query_instance.get_dataframe('adj_query')
[CPython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/localrun/glue_scripts/automation_libraries/redshift.py", line 616, in get_dataframe
[CPython39-glue_local_run] app-1                  |     conn = self.__get_conn()
[CPython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/localrun/glue_scripts/automation_libraries/redshift.py", line 424, in __get_conn
[CPython39-glue_local_run] app-1                  |     raise e
[CPython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/localrun/glue_scripts/automation_libraries/redshift.py", line 413, in __get_conn
[CPython39-glue_local_run] app-1                  |     conn = psycopg2.connect(
[CPython39-glue_local_run] app-1                  |   File "/usr/local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
[CPython39-glue_local_run] app-1                  |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[CPython39-glue_local_run] app-1                  | psycopg2.OperationalError: connection to server at "fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com" (10.15.1.101), port 5439 failed: Connection refused
[CPython39-glue_local_run] app-1                  |     Is the server running on that host and accepting TCP/IP connections?
ChatGPT said:
The error message you're getting is:

psycopg2.OperationalError: connection to server at "fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com", port 5439 failed: Connection refused
    Is the server running on that host and accepting TCP/IP connections?
üí£ Root Cause
This means that your Glue local runner is unable to connect to the Redshift cluster at:

fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com
on port 5439, because the connection was refused.

This is typically caused by network or security issues, not code issues.

‚úÖ Troubleshooting Checklist
Cause	Fix
üîí Redshift not running	Check if the Redshift cluster is up in AWS Console
üåê Glue (or local) can't reach Redshift endpoint	From your local machine (or Docker container), try:
telnet <redshift-endpoint> 5439
or
nc -vz <redshift-endpoint> 5439
üî• VPC / Subnet access issue	Ensure that Redshift is in a public subnet (or that your environment has VPC peering or VPN access)
üß± Security Group blocking access	Check that Redshift's security group allows inbound traffic from your IP or VPC subnet on port 5439
üö´ Redshift not whitelisted locally	If you‚Äôre running from your local machine, check if you need to be on a VPN or use a proxy
‚ùó Firewall or proxy blocking access	Check your corporate network or firewall rules
‚ö†Ô∏è Wrong stage/hostname/env	You‚Äôre using "BETA" as stage. Make sure the BETA cluster is the right one, and that it's accessible
üß™ How to Test
Run this from your local terminal or Docker container:

nc -vz fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com 5439
If it says:

Connection refused
Then it's a network/firewall/VPC issue, not a code bug.

üí° What You Can Do
Check AWS Console:

Go to Redshift ‚Üí Clusters ‚Üí Check if your cluster is available.

See what VPC/Subnet it‚Äôs in.

Inspect Redshift Security Group:

Inbound rule must allow port 5439 from:

Your IP (if connecting locally), or

Your Glue local Docker CIDR block (e.g., 172.17.0.0/16 if using Docker on Linux).

If on VPN / corporate network, connect to VPN.

Check Python config:
Your connection code is probably like this:

psycopg2.connect(
    dbname='...',
    user='...',
    password='...',
    host='fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com',
    port=5439
)
Make sure credentials are valid too (though this isn‚Äôt a password error ‚Äî it's network).

üß© Tip
To test whether the issue is with the host, try:

ping fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com
Or try resolving DNS:

nslookup fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com
If DNS fails ‚Äî it's a name resolution issue.
If ping or connection fails ‚Äî it's a network/firewall/VPC issue.

Let me know if you want help debugging from a Glue job perspective (e.g., testing with boto3, temporary credentials, or VPC setup).

You said:
 conn = psycopg2.connect(
[CPython39-glue_local_run] app-1                  |   File "/usr/local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
[CPython39-glue_local_run] app-1                  |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[CPython39-glue_local_run] app-1                  | psycopg2.OperationalError: connection to server at "fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com" (10.15.1.101), port 5439 failed: Connection refused
ChatGPT said:
Thanks ‚Äî this confirms it:

psycopg2.OperationalError: connection to server at "fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com" (10.15.1.101), port 5439 failed: Connection refused
This means your Glue local Docker container is trying to connect to Redshift's internal/private IP (10.x.x.x) ‚Äî and failing because:

üö® You are outside the AWS network and the Redshift cluster is private.
üîç Why This Happens:
The IP 10.15.1.101 is an internal VPC IP, meaning:

The Redshift cluster is not publicly accessible

It only allows connections from within its VPC or from whitelisted VPCs (e.g., Glue jobs in AWS)

When running locally (Glue local runner), you are outside that network

‚úÖ Fix Options
‚úÖ Option 1: Connect via VPN (if provided)
If your org has a VPN into the AWS VPC, connect to it. Once on VPN, the 10.15.1.101 IP may become reachable.

‚úÖ Option 2: Port Forward with SSH tunnel (if you have a bastion host)
If your Redshift is accessible via a bastion (EC2 instance in the VPC), you can do:

ssh -i ~/.ssh/key.pem -L 5439:redshift-endpoint:5439 ec2-user@<bastion-host>
Then connect to Redshift locally via:

conn = psycopg2.connect(
    host='localhost',
    port=5439,
    ...
)
‚úÖ Option 3: Make Redshift Publicly Accessible (temporary / not recommended for prod)
Go to Redshift ‚Üí Clusters ‚Üí [Your Cluster] ‚Üí Network and security

Enable ‚ÄúPublicly accessible‚Äù

Ensure Security Group allows port 5439 from your public IP

Get the public DNS name and try connecting again from local

‚ö†Ô∏è Security Risk if left open ‚Äî recommended only for test environments.

‚úÖ Option 4: Run the code from within AWS (Glue job / EC2)
If you can‚Äôt expose Redshift publicly and don‚Äôt have VPN or SSH, the best option is to test from inside AWS:

Upload script as a real Glue job (not local run)

Or test from an EC2 instance in same VPC

‚úÖ How to Confirm
Run from your terminal:

ping fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com
nc -vz fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com 5439
You'll most likely get:

Connection refused OR Timed out
Which confirms you‚Äôre blocked by network.

‚úÖ Final Recommendation
For AWS Glue local runs, ensure:

The Redshift endpoint is public, or

You're on VPN, or

You're using AWS port forwarding or tunneling, or

Test logic using mocked data locally, then deploy and test end-to-end inside AWS Glue

Would you like help mocking the Redshift response for local dev so you can test code paths without real DB access?

You said:
ython39-glue_local_run] app-1                  |   File "/home/glue_user/workspace/localrun/glue_scripts/automation_libraries/redshift.py", line 413, in __get_conn
[CPython39-glue_local_run] app-1                  |     conn = psycopg2.connect(
[CPython39-glue_local_run] app-1                  |   File "/usr/local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
[CPython39-glue_local_run] app-1                  |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[CPython39-glue_local_run] app-1                  | psycopg2.OperationalError: connection to server at "fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com" (10.15.1.101), port 5439 failed: Connection refused
[CPython39-glue_local_run] app-1                  |     Is the server running on that host and accepting TCP/IP connections?
[CPython39-glue_local_run] app-1                  | 
[CPython39-glue_local_run] app-1                  | with WO as (select DISTINCT source_id_int_1 as wo_source_id_int_1,distribution_id as wo_distribution_id,    source_distribution_id as wo_source_distribution_id,segment4 as wo_account_number,segment1 as wo_company_code,    segment2 as wo_location_code,segment3 as wo_cost_center,segment5 as wo_product_line,segment6 as wo_channel,    segment7 as wo_project,segment8 as wo_segment8,je_category as wo_je_category,ledger_name as wo_ledger_name,    period_name as wo_period_name,unrounded_accounted as wo_unrounded_accounted from fast.gl_ar_linkage where segment4 = '20321' and     ledger_name='Amazon.com, Inc.' and je_category = 'Adjustment' AND upper(period_name)='JUN-25'),    AR as (select DISTINCT customer_name as ar_vendor,invoice_number as ar_invoice_num,invoice_date as ar_invoice_date,    adjustment_id as ar_adjustment_id, adjustment_amount as ar_adjustment_amount, product_code as ar_product_code,    source_distribution_id as ar_source_distribution_id, ledger_name as ar_ledger_name from fdv_ext.ar_adjustment_distribution_details where account_number  IN ('20320','13113')     AND ledger_name='Amazon.com, Inc.'), BASE as (select * from WO left join AR on     wo.wo_source_id_int_1 = ar.ar_adjustment_id) select wo_source_id_int_1, wo_distribution_id,     wo_source_distribution_id,wo_ledger_name,wo_period_name, wo_je_category, wo_company_code,wo_location_code,     wo_cost_center, wo_account_number, wo_product_line, wo_channel, wo_project, wo_segment8, wo_unrounded_accounted,    ar_adjustment_id, ar_invoice_date, ar_invoice_num, ar_ledger_name, ar_vendor, ar_product_code,     ar_adjustment_amount, ar_source_distribution_id from BASE
[CPython39-glue_local_run] app-1                  | BETA
ChatGPT said:
The error remains clear and consistent:

‚ùå psycopg2.OperationalError: connection refused
connection to server at "fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com" (10.15.1.101), port 5439 failed: Connection refused
üîÅ Summary of What‚Äôs Happening
You're running a Glue local job, and it's trying to connect to Redshift using a private VPC endpoint (10.15.x.x), which is not accessible from your local network or machine.

This is not a problem with the SQL query (you've shown a full, valid query).

‚úÖ Confirmed Root Cause
You're outside the AWS network

Redshift is private, not publicly accessible

Redshift security group likely only allows access from:

Glue jobs running inside AWS

Specific VPCs or EC2 instances

You‚Äôre running a local Glue container ‚Äî which doesn't have VPC access

‚úÖ Your Options
‚úÖ 1. Test inside real AWS Glue job (Recommended)
Deploy and run your script in AWS Glue, where the Redshift cluster is reachable.

‚úÖ 2. Connect to VPN (if your org provides one)
If you're on a corporate VPN that connects to the VPC, try:

nc -vz fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com 5439
Success here = you're able to run locally now.

‚úÖ 3. SSH Tunnel (advanced but safe)
If there's a bastion EC2 host in the VPC, you can SSH tunnel:

ssh -i ~/.ssh/key.pem -L 5439:redshift-endpoint:5439 ec2-user@bastion-host
Then in code:

conn = psycopg2.connect(
    host='localhost',
    port=5439,
    ...
)
‚ö†Ô∏è 4. Temporarily make Redshift Public (Not recommended for prod)
Only if it's a non-prod cluster:

Go to Redshift ‚Üí Cluster ‚Üí Networking

Enable ‚ÄúPublicly accessible‚Äù

Add your IP to the security group on port 5439

Try again from local machine

üß™ Quick Diagnostic to Prove It's a Network Issue
Run from your local terminal:

nslookup fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com
Then:

ping fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com

nc -vz fast-platform-infra-beta-rscluster.cmzplfw6m1ou.us-east-1.redshift.amazonaws.com 5439
All of these will likely fail with timeout or refused, confirming no network route.

‚úÖ How to Keep Developing Locally
If you're blocked and want to continue working:

Use Mocked DataFrames:
In your local runner, instead of querying Redshift:

if IS_LOCAL_RUN:
    adjs = pd.read_csv("mock/adjs.csv")
else:
    adjs = adj_query_instance.execute_query('adj_query')
Same with others (cms, pis, rvr) ‚Äî just mock data and test the flow.

üí¨ TL;DR
Your Glue local container cannot reach a private Redshift cluster in AWS.
Either move testing into AWS Glue, connect via VPN, use SSH tunneling, or mock your data.

Let me know which option you'd like help setting up (mock data, SSH tunnel, VPN, or Glue job deployment).





No file chosenNo file chosen
ChatGPT can make mistakes. Check important info. See Cookie Preferences.
